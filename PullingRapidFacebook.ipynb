{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c75567-e456-4550-a77b-044241963591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Function to fetch and append data to CSV\n",
    "def fetch_and_append_data(query, location_uid, csv_filename):\n",
    "    url = \"https://facebook-scraper3.p.rapidapi.com/search/posts\"\n",
    "    querystring = {\"query\": query, \"location_uid\": location_uid}\n",
    "\n",
    "    headers = {\n",
    "        \"x-rapidapi-host\": \"facebook-scraper3.p.rapidapi.com\",\n",
    "        \"x-rapidapi-key\": \"c7d63f413cmshf2851df4a5e42d1p1ed1bejsn092d86a02e69\"\n",
    "    }\n",
    "\n",
    "    # Open CSV for appending data (not overwriting)\n",
    "    with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write headers only once if the file is empty\n",
    "        file.seek(0, 2)  # Move to the end of the file\n",
    "        if file.tell() == 0:  # Check if the file is empty\n",
    "            writer.writerow([\n",
    "                'post_id', 'type', 'message', 'timestamp', 'comments_count', 'reactions_count', 'author_name', \n",
    "                'author_profile', 'author_profile_picture', 'media_type', 'media_url', 'reaction_like', 'reaction_love',\n",
    "                'reaction_sad', 'reaction_anger', 'reaction_haha', 'reaction_wow'\n",
    "            ])\n",
    "            print(f\"Writing headers to {csv_filename} as file is empty.\")\n",
    "\n",
    "        last_post_timestamp = None  # Track the last post timestamp\n",
    "\n",
    "        while True:\n",
    "            print(f\"Fetching new data for query: '{query}' with location UID: '{location_uid}'\")\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, params=querystring)\n",
    "                response.raise_for_status()  # Raise error if response is not 200\n",
    "                data = response.json()\n",
    "\n",
    "                # Check if 'results' exists and write new posts\n",
    "                if data.get('results'):\n",
    "                    for post in data['results']:\n",
    "                        post_id = post.get('post_id', '')\n",
    "                        post_type = post.get('type', '')\n",
    "                        message = post.get('message', '').replace('\\n', ' ').replace('\\r', '')\n",
    "                        timestamp = post.get('timestamp', '')\n",
    "                        comments_count = post.get('comments_count', 0)\n",
    "                        reactions_count = post.get('reactions_count', 0)\n",
    "                        author_name = post.get('author', {}).get('name', '')\n",
    "                        author_profile = post.get('author', {}).get('profile', '')\n",
    "                        author_profile_picture = post.get('author', {}).get('profile_picture_url', '')\n",
    "                        \n",
    "                        # Handle media info\n",
    "                        media_type = \"\"\n",
    "                        media_url = \"\"\n",
    "                        if post.get('media'):\n",
    "                            media_type = post['media'].get('type', '')\n",
    "                            media_url = post['media'].get('url', '')\n",
    "                        \n",
    "                        # Reactions\n",
    "                        reactions = post.get('reactions', {})\n",
    "                        reaction_like = reactions.get('like', 0)\n",
    "                        reaction_love = reactions.get('love', 0)\n",
    "                        reaction_sad = reactions.get('sad', 0)\n",
    "                        reaction_anger = reactions.get('anger', 0)\n",
    "                        reaction_haha = reactions.get('haha', 0)\n",
    "                        reaction_wow = reactions.get('wow', 0)\n",
    "\n",
    "                        # Only write if new post based on timestamp or post_id\n",
    "                        if last_post_timestamp is None or timestamp > last_post_timestamp:\n",
    "                            writer.writerow([\n",
    "                                post_id, post_type, message, timestamp, comments_count, reactions_count, author_name, \n",
    "                                author_profile, author_profile_picture, media_type, media_url, reaction_like, reaction_love,\n",
    "                                reaction_sad, reaction_anger, reaction_haha, reaction_wow\n",
    "                            ])\n",
    "\n",
    "                            # Update the last timestamp to track the most recent post\n",
    "                            last_post_timestamp = timestamp\n",
    "                            print(f\"New post added: {post_id}\")\n",
    "\n",
    "                # Check if there is a cursor to get the next page of data\n",
    "                cursor = data.get('cursor')\n",
    "                if cursor:\n",
    "                    querystring['cursor'] = cursor\n",
    "                    print(\"Moving to next page...\")\n",
    "                else:\n",
    "                    print(\"No more data. Exiting.\")\n",
    "                    break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request error: {e}. Retrying in 30 seconds.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}. Retrying in 30 seconds.\")\n",
    "\n",
    "            # Sleep for a short interval to prevent constant polling (e.g., 86400 seconds - 24 hours)\n",
    "            time.sleep(86400)\n",
    "\n",
    "# Example usage: continuously fetch and append data related to disaster keywords\n",
    "disaster_keywords = [\"flood\", \"earthquake\", \"hurricane\", \"fire\", \"storm\", \"tsunami\", \"disaster\", \"banjir\", \"gempa\", \"taufan\"]\n",
    "\n",
    "for keyword in disaster_keywords:\n",
    "    fetch_and_append_data(keyword, \"malaysia\", \"facebook_posts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ef3a7-0ec0-47f6-8f50-9e0e7cdb31ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
